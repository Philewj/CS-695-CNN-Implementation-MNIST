{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCWxo8eo5qGmcbvpLAq8lI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Philewj/CS-695-CNN-Implementation-MNIST/blob/main/CS_695_Project2_CNN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project: Implementing CNN for the MNIST Dataset"
      ],
      "metadata": {
        "id": "t__Wtr9GZqkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Model"
      ],
      "metadata": {
        "id": "lxPG0WjoaG5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "BRhZDdkya5tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "zy5cbnDHamLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Covert MNIST Image Files into Tensors of 4 Dimensions (# of images, Height, Width, Color Channels)\n",
        "transform = transforms.ToTensor()\n",
        "#Load Training Data\n",
        "train_data = datasets.MNIST(root='/cnn_data', train=True, download=True, transform=transform)\n",
        "#Load Testing Data\n",
        "test_data = datasets.MNIST(root='/cnn_data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "0PoQl4GvaFhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggTQlYW0ZNmQ",
        "outputId": "510057fa-848f-4b40-d453-7729b623c448"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: /cnn_data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#Verifying Training Data\n",
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Verifying Testing Data\n",
        "test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBIlDT64b6if",
        "outputId": "25ca14b8-08b8-4e64-e139-ec6a552c7e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 10000\n",
              "    Root location: /cnn_data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Model"
      ],
      "metadata": {
        "id": "-yhYpMBpdimN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Class\n",
        "class ConvolutionalNeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Create 3 Convolutional Layers\n",
        "    self.convolution_layer1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3,stride=1)\n",
        "    self.convolution_layer2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3,stride=1)\n",
        "\n",
        "    #Fully Connected Neural Layer3\n",
        "    self.fc1 = nn.Linear(in_features=5*5*16, out_features=120)\n",
        "    self.fc2 = nn.Linear(in_features=120, out_features=85) # decrease the number of out features (can be an arbitrary number)\n",
        "    self.fc3 = nn.Linear(in_features=85, out_features=55) # is the number of classes in the dataset\n",
        "    self.fc4 = nn.Linear(in_features=55, out_features=10) # is the number of classes in the dataset\n",
        "\n",
        "  def forward(self, X):\n",
        "    # Pass through Convolutional Layer 1\n",
        "    X = F.relu(self.convolution_layer1(X))\n",
        "    X = F.max_pool2d(X, 2, 2)\n",
        "    # Pass through Convolutional Layer 2\n",
        "    X = F.relu(self.convolution_layer2(X))\n",
        "    X = F.max_pool2d(X, 2, 2)\n",
        "\n",
        "\n",
        "    #Re-View data to flatten it out\n",
        "    X = X.view(-1, 16*5*5) # -1 so we can vary batch size\n",
        "\n",
        "    #Fully Connected Layers\n",
        "    X = F.relu(self.fc1(X))\n",
        "    X = F.relu(self.fc2(X))\n",
        "    X = F.relu(self.fc3(X))\n",
        "    X = self.fc4(X)\n",
        "    return F.log_softmax(X, dim=1)"
      ],
      "metadata": {
        "id": "7tPvmaUxd0I3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create an Intance of the Model\n",
        "#Create a manual seed\n",
        "torch.manual_seed(41)\n",
        "model = ConvolutionalNeuralNetwork()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWnLyzf5fnB-",
        "outputId": "d9b59ecb-85cf-482f-bc47-e753bd426cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvolutionalNeuralNetwork(\n",
              "  (convolution_layer1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (convolution_layer2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=85, bias=True)\n",
              "  (fc3): Linear(in_features=85, out_features=55, bias=True)\n",
              "  (fc4): Linear(in_features=55, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss Function Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "2HWpphccf1eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Model"
      ],
      "metadata": {
        "id": "_sRXWFCQgP2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Keep track of how long it takes to train the model\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "#Create a small batch size for images ...10\n",
        "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=10, shuffle=False)\n",
        "\n",
        "#Create Variables to track things\n",
        "epochs = 5\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_correct = []\n",
        "test_correct = []\n",
        "\n",
        "\n",
        "#Create For Loops of Epochs\n",
        "for i in range(epochs) :\n",
        "  trn_corr = 0\n",
        "  tst_corr = 0\n",
        "\n",
        "  #Train\n",
        "  for b, (X_train, y_train) in enumerate(train_loader):\n",
        "    b+=1 #start our batches at 1\n",
        "    y_pred = model(X_train)   # Get predicted values from training set. Not Flattened.2D\n",
        "    loss = criterion(y_pred, y_train) # Calculate Loss, Compare the predictions to correct answers in y_train\n",
        "\n",
        "    predicted = torch.max(y_pred.data,1)[1] #add up the number of correct predictions\n",
        "    batch_corr = (predicted == y_train).sum() # How many we got correct from this batch. True =1, False = 0, sum those up\n",
        "    trn_corr += batch_corr #Keep track as we go along in training\n",
        "\n",
        "\n",
        "    #Update our parameters\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "    #Print results\n",
        "    if b%600 ==0 :\n",
        "      print(f'Epoch: {i}  Batch: {b}  Loss: {loss.item()}')\n",
        "  train_losses.append(loss)\n",
        "  train_correct.append(trn_corr)\n",
        "\n",
        "  #Test\n",
        "  with torch.no_grad():  # No Gradient so that we don't update our weights and biases with test data\n",
        "    for b, (X_test, y_test) in enumerate(test_loader):\n",
        "      y_val = model(X_test)\n",
        "      predicted = torch.max(y_val.data, 1)[1] #Adding up correct predictions\n",
        "      tst_corr += (predicted == y_test).sum() #True =1 False = 0 and sum them\n",
        "\n",
        "\n",
        "    loss = criterion(y_val, y_test)\n",
        "    test_losses.append(loss)\n",
        "    test_correct.append(tst_corr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "current_time = time.time()\n",
        "total_time = current_time - start_time\n",
        "print(f'Training Took: {total_time/60}  minutes!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxqVi61cgdbf",
        "outputId": "55cb4be0-e2f8-4eef-bd0c-f3a99471fadb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0  Batch: 600  Loss: 0.6502459049224854\n",
            "Epoch: 0  Batch: 1200  Loss: 0.19238707423210144\n",
            "Epoch: 0  Batch: 1800  Loss: 0.199004128575325\n",
            "Epoch: 0  Batch: 2400  Loss: 0.1920306235551834\n",
            "Epoch: 0  Batch: 3000  Loss: 0.037895798683166504\n",
            "Epoch: 0  Batch: 3600  Loss: 0.0013977289199829102\n",
            "Epoch: 0  Batch: 4200  Loss: 0.19188503921031952\n",
            "Epoch: 0  Batch: 4800  Loss: 0.014569433405995369\n",
            "Epoch: 0  Batch: 5400  Loss: 0.034884121268987656\n",
            "Epoch: 0  Batch: 6000  Loss: 0.10975992679595947\n",
            "Epoch: 1  Batch: 600  Loss: 0.0022547177504748106\n",
            "Epoch: 1  Batch: 1200  Loss: 0.032249368727207184\n",
            "Epoch: 1  Batch: 1800  Loss: 0.028194641694426537\n",
            "Epoch: 1  Batch: 2400  Loss: 0.015163414180278778\n",
            "Epoch: 1  Batch: 3000  Loss: 0.15465423464775085\n",
            "Epoch: 1  Batch: 3600  Loss: 0.0036955750547349453\n",
            "Epoch: 1  Batch: 4200  Loss: 0.010395861230790615\n",
            "Epoch: 1  Batch: 4800  Loss: 0.10470445454120636\n",
            "Epoch: 1  Batch: 5400  Loss: 0.09734562784433365\n",
            "Epoch: 1  Batch: 6000  Loss: 0.09871844947338104\n",
            "Epoch: 2  Batch: 600  Loss: 0.0002710794797167182\n",
            "Epoch: 2  Batch: 1200  Loss: 0.0001686671021161601\n",
            "Epoch: 2  Batch: 1800  Loss: 0.022515928372740746\n",
            "Epoch: 2  Batch: 2400  Loss: 0.0010719847632572055\n",
            "Epoch: 2  Batch: 3000  Loss: 0.0022705583833158016\n",
            "Epoch: 2  Batch: 3600  Loss: 0.00038400464109145105\n",
            "Epoch: 2  Batch: 4200  Loss: 0.002982970094308257\n",
            "Epoch: 2  Batch: 4800  Loss: 0.0007147907163016498\n",
            "Epoch: 2  Batch: 5400  Loss: 0.06090885400772095\n",
            "Epoch: 2  Batch: 6000  Loss: 0.2739656865596771\n",
            "Epoch: 3  Batch: 600  Loss: 0.02408997155725956\n",
            "Epoch: 3  Batch: 1200  Loss: 0.00010644793655956164\n",
            "Epoch: 3  Batch: 1800  Loss: 0.0008717809105291963\n",
            "Epoch: 3  Batch: 2400  Loss: 0.01020282693207264\n",
            "Epoch: 3  Batch: 3000  Loss: 0.010159932076931\n",
            "Epoch: 3  Batch: 3600  Loss: 0.007908409461379051\n",
            "Epoch: 3  Batch: 4200  Loss: 0.0033777286298573017\n",
            "Epoch: 3  Batch: 4800  Loss: 0.07698921114206314\n",
            "Epoch: 3  Batch: 5400  Loss: 0.00017766683595255017\n",
            "Epoch: 3  Batch: 6000  Loss: 0.00011563345469767228\n",
            "Epoch: 4  Batch: 600  Loss: 0.0013819817686453462\n",
            "Epoch: 4  Batch: 1200  Loss: 0.08414387702941895\n",
            "Epoch: 4  Batch: 1800  Loss: 0.003746108617633581\n",
            "Epoch: 4  Batch: 2400  Loss: 6.067120557418093e-05\n",
            "Epoch: 4  Batch: 3000  Loss: 0.00743964035063982\n",
            "Epoch: 4  Batch: 3600  Loss: 9.202759429172147e-06\n",
            "Epoch: 4  Batch: 4200  Loss: 0.001506699831224978\n",
            "Epoch: 4  Batch: 4800  Loss: 0.05974804237484932\n",
            "Epoch: 4  Batch: 5400  Loss: 0.00885892566293478\n",
            "Epoch: 4  Batch: 6000  Loss: 0.18680649995803833\n",
            "Training Took: 3.207930338382721  minutes!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Training 2"
      ],
      "metadata": {
        "id": "6QTWdCgoxPcs"
      }
    }
  ]
}